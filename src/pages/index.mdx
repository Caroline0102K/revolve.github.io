---
layout: ../layouts/Layout.astro
title: Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: favicon.svg
thumbnail: screenshot.png
---
import WarningWithCheckbox from '../components/Warnings.astro';
import Layout from "../layouts/Layout.astro";
import GetInvolved from "../components/involved.astro";
import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import Example from '../components/Examples.astro';
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import poster from "../assets/poster.PNG";
import outside from "../assets/outside.mp4";
import intro1029 from "../assets/intro1029.png";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
export const components = {pre: CodeBlock}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Haibo Jin",
      url: "https://haibojin001.github.io/",
      institution: "School of Information Sciences \n University of Illinois at Urbana-Champaign",
    },
    {
      name: "Andy Zhou",
      url: "https://www.andyzhou.ai/",
      institution: "Computer Science \n Lapis Labs \n University of Illinois Urbana-Champaign",
    },
    {
      name: "Joe D. Menke",
      url: "https://github.com/jomenke",
      institution: "School of Information Sciences \n University of Illinois at Urbana-Champaign",
    },
    {
      name: "Haohan Wang",
      url: "https://haohanwang.github.io/",
      institution: "School of Information Sciences \n University of Illinois at Urbana-Champaign",
      notes: ["*"],
    },
  ]}
  conference="Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)"
  notes={[
    {
      symbol: "*",
      text: "Corresponding Author",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://arxiv.org/pdf/2405.20413",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code",
      url: "https://github.com/Allen-piexl/llm_moderation_attack",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2405.20413",
      icon: "academicons:arxiv",
    },
  ]}
  />

 {/*<Video source={outside} />*/}

<HighlightedSection>

## Abstract

Large Language Models (LLMs) are typically harmless but remain vulnerable to carefully crafted prompts known as "jailbreaks", which can bypass protective measures and induce harmful behavior. Recent advancements in LLMs have incorporated moderation guardrails that can filter outputs, which trigger processing errors for certain malicious questions. Existing red-teaming benchmarks often neglect to include questions that trigger moderation guardrails, making it difficult to evaluate jailbreak effectiveness. To address this issue, we introduce JAMBench, a harmful behavior benchmark designed to trigger and evaluate moderation guardrails. JAMBench involves 160 manually crafted instructions covering four major risk categories at multiple severity levels. Furthermore, we propose a jailbreak method, JAM (Jailbreak Against Moderation), designed to attack moderation guardrails using jailbreak prefixes to bypass input-level filters and a fine-tuned shadow model functionally equivalent to the guardrail model to generate cipher characters to bypass output-level filters. Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success (~ × 19.88) and lower filtered-out rates (~ × 1/6) than baselines.

</HighlightedSection>



## Motivation

Examples of jailbreaks. (a) A malicious question that receives a refusal response from the LLM. (b) An affirmative response with detailed steps to implement the malicious question by adding a jailbreak prompt as the prefix. (c) A filtered-out error is triggered by the moderation guardrail, even when a successful jailbreak prompt is added. (d) An affirmative response using JAM, which combines a jailbreak prefix, the malicious question, and the cipher characters to bypass the guardrail.

<Figure
    caption=""
  >
    <Image source={intro1029} altText="Diagram of the transformer deep learning architecture." />
</Figure>

## Examples

<WarningWithCheckbox>
  <Example />
</WarningWithCheckbox>

## Paper Showcase

The presentation video on the left explains the problem addressed, our methodology, and key outcomes, helping viewers understand the broader impact of our work. On the right, the poster offers a visual summary of major findings and innovations, designed to capture the core essence of our research at a glance.

<TwoColumns>
  <Figure slot="left" caption="Watch our paper presentation video.">
    <YouTubeVideo videoId="E2P6XKi6HMs" />
  </Figure>
  <Figure slot="right" caption="Here is the poster for our paper.">
    {/*<Splat client:idle />*/}
   <Image source={poster} altText="Diagram of the transformer deep learning architecture." />
  </Figure>
</TwoColumns>


## Get Involved
<GetInvolved />

## BibTeX citation

```bibtex
@article{jin2024jailbreaking,
  title={Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters},
  author={Jin, Haibo and Zhou, Andy and Menke, Joe D and Wang, Haohan},
  journal={arXiv preprint arXiv:2405.20413},
  year={2024}
}
```
